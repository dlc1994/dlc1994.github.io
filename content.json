{"meta":{"title":"DLC's Blog","subtitle":"Enjoy Typing!","description":"Personal blogs","author":"Lingcheng Dai","url":"https://dlc1994.github.io","root":"/"},"pages":[{"title":"戴凌成（Lingcheng Dai)","date":"2019-03-17T16:26:13.000Z","updated":"2019-03-18T06:34:48.339Z","comments":false,"path":"about/index.html","permalink":"https://dlc1994.github.io/about/index.html","excerpt":"","text":"个人信息 手机：(+86)15321487166 Email：dailingcheng@foxmail.com 技术博客：https://dlc1994.github.io Github: https://github.com/dlc1994 教育经历北京邮电大学 信息与通信工程学院 信息与通信工程 工学硕士 2017.09-2020.06北京邮电大学 信息与通信工程学院 通信工程 学士 2013.09-2017.06 竞赛经历中国“易华录杯”CCF软件服务创新大赛——公交线路准点预测 三等奖 2018.09对天津市一个月5716辆公交车超过4亿条GPS记录进行数据处理，构建天气、日期等特征，调用Scikit-learn中的XGBoost回归模型对错分样本进行不断加权，预测得到的公交车运行时间与平均值线性叠加后乘以拥堵系数，最后给出每辆公交车在各个站点间的运行时间。 北邮、北师、中农大校园大数据竞赛——校园人流量预测 5% 2018.03提取校园内33个地点10个月内采集的所有手机终端位置的数据，利用数据挖掘平台WEKA，对常用时间序列分析模型比如ARIMA,ARCH等进行分析，考虑课程、节假日对人流量的影响，不断修正预测模型及模型参数，最终对11月和12月校园内33个地点每小时的人数进行预测。 工作经历北京佰才帮技术有限公司 协议栈开发工程师 2019.02 - 2019.03工作任务 负责5G/LTE小基站上Layer2/Layer3（MAC、PDCP、RLC和RRC等）协议栈的研究、设计工作； 负责在CentOS上编写Makefile完成数十万行C代码工程的维护、编译和静态库封装； 负责完成一些代码整合、移植和混淆工作。 科研项目华为HIRP项目“基于机器学习的免传模一键式站址规划” 2018.01 - 2019.01项目简述独立完成从原始数据文件中用python提取数据，利用开源库Scikit-learn中的机器学习算法比如SVM、k近邻、神经网络等，在CentOS服务器上训练得到电平预测模型，最后利用遗传算法（开源框架GAFT）和贪心算法对基站覆盖率进行在线优化，确定基站最优部署参数。工程代码达上千行。 华为HIRP项目“以用户为中心网络下的资源管理和干扰协同研究” 2017.09-2018.08项目简述独立完成在以用户为中心的多天线超密组网下，基于WMMSE设计了一种低复杂度的分布式聚簇和波束赋形算法，通过块对角化方法将原始NP-hard非凸问题分解为各簇各变量的凸优化问题，再采用拉格朗日乘数法求解，最后在Matlab上进行系统级仿真，提升了边缘用户速率。 中兴项目“超密组网下的用户移动性预测研究” 2017.08-2018.06项目简述独立完成对现有学术界移动性预测方案的调研，对当前移动性预测采用的方案进行了详细总结，包括（隐）马尔科夫过程、贝叶斯网络、数据挖掘和神经网络等，并提出移动性预测在未来5G及6G路径规划、移动性优化、辅助定位上的应用和挑战，以及深度学习可能带来的巨大性能提升。 发表作品&lt;学生一作&gt;, “Mobility Prediction: A Survey on State-of-the-Art Schemes and Future Applications,” IEEE Access, vol. 7, pp. 802-822, 2019.（SCI，影响因子3.557）&lt;第一作者&gt;, “Propagation-model-free Coverage Evaluation via Machine Learning for Future 5G Networks,” in IEEE PIMRC, Italy, 2018, pp. 1-5.（EI，B类会议） 获奖经历2017-2018 一等学业奖学金2015-2016 国家励志奖学金、校“三好学生”、优秀共青团员2014.05 北京邮电大学“明日之星”英语风采大赛 配音组二等奖 技能 程序开发：C/C++/Python/Matlab 排版编辑：LaTex/Markdown/MS Office 操作系统：Linux/Windows 致谢感谢您花时间阅读我的简历，期待能有机会和您共事。"},{"title":"LeetCode--Two Sum","date":"2018-07-30T04:49:07.000Z","updated":"2018-07-30T04:50:52.605Z","comments":true,"path":"drafts/LeetCode--Two Sum.html","permalink":"https://dlc1994.github.io/drafts/LeetCode--Two Sum.html","excerpt":"","text":"Description of Problem: Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].&gt; Approach 1: Brute Force1234for i in range(0,len(nums)-1): for j in range(i+1,len(nums)): if nums[i]+nums[j]==target: return [i,j] running time: 5640ms Approach 2: Python List.index1234for i in range(len(nums)): if (target-nums[i]) in nums: if (nums.index(target - nums[i])) != i: return [i, nums.index(target - nums[i])] running time: 1072ms Approach 3: Python Dict123456dict = &#123;&#125;for i in range(len(nums)): if (target-nums[i]) in dict: return [dict[(target-nums[i])], i] else: dict[nums[i]]=i running time: 40ms Approach 4: Enumerate+Hash_Table12345hash_table=&#123;&#125;for i, value in enumerate (nums): if target-value in hash_table: return hash_table[target-value], i hash_table[value]=i running time: 40ms"},{"title":"Categories","date":"2018-06-12T01:44:28.000Z","updated":"2018-08-02T11:13:54.645Z","comments":false,"path":"categories/index.html","permalink":"https://dlc1994.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-06-12T02:25:00.000Z","updated":"2018-08-02T11:16:43.890Z","comments":false,"path":"tags/index.html","permalink":"https://dlc1994.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2019年3月16号今日头条暑期实习招聘笔试第一批(更新ing)","slug":"toutiao","date":"2019-03-16T13:27:23.000Z","updated":"2019-03-18T08:31:04.871Z","comments":true,"path":"toutiao.html","link":"","permalink":"https://dlc1994.github.io/toutiao.html","excerpt":"实习笔试试题，有些没投（准备好先），拿过来先看了看","text":"实习笔试试题，有些没投（准备好先），拿过来先看了看 12 头条笔试题1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;numeric&gt;using namespace std;int main()&#123; int N; cin&gt;&gt;N; for(int n=0;n&lt;N;++n)&#123; int L, minVal, minPos; minVal = 1000; minPos = 0; cin&gt;&gt;L; vector&lt;int&gt; v(L, 0); for(int l=0;l&lt;L;l++)&#123; cin&gt;&gt;v[l]; if(v[l]&lt;minVal)&#123; minVal = v[l]; minPos = l; &#125; &#125; vector&lt;int&gt; reward(v.size(), 0); reward[minPos] = 1; for(int k=0;k&lt;L;k++)&#123; int pos = k+minPos; if (pos&gt;=L)&#123; pos = pos - L; &#125; if(pos==0)&#123; if(v[pos]&gt;v[L-1]) reward[pos] = reward[L-1]+1; else if(v[pos]==v[L-1]) reward[pos] = reward[L-1] - 1; else reward[pos]=reward[pos]; &#125; else&#123; if(v[pos]&gt;v[pos-1]) reward[pos] = reward[pos-1]+1; else if(v[pos]==v[pos-1]) reward[pos] = reward[pos-1] - 1; else reward[pos]=reward[pos]; &#125; &#125; cout&lt;&lt;accumulate(reward.begin(),reward.end(),0)&lt;&lt;endl; &#125; return 0;&#125; 3 头条笔试题1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;numeric&gt;using namespace std;vector&lt;int&gt; zhaoqian(int N)&#123; vector&lt;int&gt; v(4,0); N = 1024 - N; int value[4] = &#123;64, 16, 4, 1&#125;; for(int i=0;i&lt;(sizeof(value)/sizeof(value[0]));i++)&#123; cout&lt;&lt;i&lt;&lt;endl; int a = N/value[i]; v[i] = a; N = N%value[i]; &#125; return v;&#125;int main()&#123; int N; cin&gt;&gt;N; vector&lt;int&gt; vv; vv = zhaoqian(N); cout&lt;&lt;accumulate(vv.begin(), vv.end(), 0)&lt;&lt;endl; return 0;&#125; 4 招行笔试题题目描述：切不等高的蛋糕，蛋糕N份，每刀切平一部分，切下来的不得超过k份，整块蛋糕切平最少需要几刀，比如如下输入5 61 2 3 4 5提示：第一刀可以切到2，切下来刚好是6份，剩下蛋糕为1 2 2 2 2第二刀全切到1，最少需要2刀12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;math.h&gt;#include &lt;iostream&gt;#include &lt;numeric&gt;#include &lt;algorithm&gt;#include &lt;string&gt;#include &lt;set&gt;#include &lt;map&gt;#include &lt;queue&gt;#include &lt;list&gt;#include &lt;stack&gt;#include &lt;vector&gt;using namespace std;int main()&#123; int N,k; cin&gt;&gt;N&gt;&gt;k; vector&lt;int&gt; v(N, 0); for(int l=0;l&lt;N;l++)&#123; cin&gt;&gt;v[l]; &#125; sort(v.begin(),v.end()); int minVal = v.front(); int maxVal = v.back(); int dao = 0; bool hello = false; int i = 0; while(i&lt;v.size())&#123; if(hello) i=0; if(minVal==maxVal) break; int sum = 0; bool flag = true; int minVal1=v[i]; for(int j=v.size()-1;j&gt;i;--j)&#123; sum = sum+(v[j]-minVal1); if(sum&gt;k)&#123; flag = false; break; &#125; &#125; if(flag)&#123; dao++; hello = true; for(int g=v.size()-1;g&gt;i;g--)&#123; v[g] = v[g]-(v[g]-minVal1); &#125; &#125; maxVal=v.back(); i++; &#125; cout&lt;&lt;dao; return 0;&#125;","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dlc1994.github.io/categories/LeetCode/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dlc1994.github.io/tags/C/"}]},{"title":"二叉树常用操作的实现","slug":"BinaryTree","date":"2019-03-09T12:23:52.000Z","updated":"2019-03-17T10:08:54.601Z","comments":true,"path":"BinaryTree.html","link":"","permalink":"https://dlc1994.github.io/BinaryTree.html","excerpt":"二叉树的最全总结，常考题型，持续更新","text":"二叉树的最全总结，常考题型，持续更新 1.1 二叉树的初始化12345678910111213141516171819202122#initial of BinaryTreeclass BinaryTree: def __init__(self,rootObj): self.val = rootObj self.left = None self.right = None def insertLeft(self,newNode): if self.left == None: self.left = BinaryTree(newNode) else: t = BinaryTree(newNode) t.left = self.left self.left = t def insertRight(self,newNode): if self.right == None: self.right = BinaryTree(newNode) else: t = BinaryTree(newNode) t.right = self.right self.right = t 1.2 创建一个二叉树1234567891011121314151617#create a BinaryTree# 18# 7 11# 3 4 5 6# 1 3 2 4root = BinaryTree(18)root.left = BinaryTree(7)root.right = BinaryTree(11)root.left.left = BinaryTree(3)root.left.right = BinaryTree(4)root.right.left = BinaryTree(5)root.right.right = BinaryTree(6)root.right.left.left = BinaryTree(1)root.right.left.right = BinaryTree(3)root.right.right.left = BinaryTree(2)root.right.right.right = BinaryTree(4) 1.3 前序遍历1234567891011121314151617181920212223#递归版本def PreOrder(self, node): if node: print(node.val) self.PreOrder(node.left) self.PreOrder(node.right)#循环版本def PreOrderLoop(self, node): if node == None: return stack =[] print(node.val) stack.append(node) node = node.left while stack!=[] or node: while node: print(node.val) stack.append(node) node = node.left node = stack[-1].right stack.pop()#ouput: 18 7 3 4 11 5 1 3 6 2 4 1.4 中序遍历123456789101112131415161718192021#递归版本def InOrder(self, node): if node: self.InOrder(node.left) print(node.val) self.InOrder(node.right)#循环版本def InOrderLoop(self, node): if node == None: return None stack = [] stack.append(node) node = node.left while stack!=[] or node: while node: stack.append(node) node = node.left print(stack[-1].val) node = stack[-1].right stack.pop()#output：3 7 4 18 1 5 3 11 2 6 4 1.5 后序遍历1234567891011121314151617181920212223242526#递归def PostOrder(self, node): if node: self.PostOrder(node.left) self.PostOrder(node.right) print(node.val)#非递归def PostOrderLoop(self, node): if node == None: return stack =[] stack.append(node) pre = None while stack!=[]: node = stack[-1] if ((node.left==None and node.right==None) or (pre and (pre == node.left or pre ==node.right))): print(node.val) pre = node stack.pop() else: if node.right: stack.append(node.right) if node.left: stack.append(node.left)#output:3 4 7 1 3 5 2 4 6 11 18 1.6 层序遍历1234567891011121314def LevelOrder(self, node): if node == None: return stack = [] stack.append(node) while stack!=[]: node = stack[0] if node.left: stack.append(node.left) if node.right: stack.append(node.right) print(node.val) stack.pop(0)output: 18 7 11 3 4 5 6 1 3 2 4 1.7 计算节点数1234567891011121314151617181920#递归版本def CountNode(self, root): if root == None: return 0 return self.CountNode(root.left) + self.CountNode(root.right) + 1#非递归版本def CountNodeNotRev(self, root): if root == None: return 0 stack = [] stack.append(root) index = 0 while index&lt;len(stack): if stack[index].left: stack.append(stack[index].left) if stack[index].right: stack.append(stack[index].right) index += 1 print(len(stack))output: 11","categories":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://dlc1994.github.io/categories/Data-Structure/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dlc1994.github.io/tags/Python/"},{"name":"Binary Tree","slug":"Binary-Tree","permalink":"https://dlc1994.github.io/tags/Binary-Tree/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-08T12:14:55.325Z","updated":"2018-07-21T08:05:26.801Z","comments":true,"path":"hello-world.html","link":"","permalink":"https://dlc1994.github.io/hello-world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Linear Regression","slug":"LinearRegression","date":"2018-11-28T12:23:52.000Z","updated":"2018-11-28T14:49:15.246Z","comments":true,"path":"LinearRegression.html","link":"","permalink":"https://dlc1994.github.io/LinearRegression.html","excerpt":"","text":"就从最简单的线性回归模型（Linear Regression model）开始学习吧。 从这个模型的名字我们可以看出，因变量和变量之间的关系是线性的，预测值可以通过计算输入特征的权重给出： \\hat{y} = \\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n其中，$\\hat{y}$是预测值，$n$是特征数量，$\\theta_0$是bias，$x_i$是第$i$个特征值，$\\theta_j$是第$j$个特征的模型参数。如果获取了模型的所有参数，给定一个样本我们就可以用上面这个公式模型得到预测值。 为了表达更简洁，一般用向量表示： \\hat{y} = h_{\\mathbf{\\theta}}(\\mathbf{x}) = \\mathbf{\\theta}^T \\mathbf{x}其中$h_{\\mathbf{\\theta}}()$是hypothesis function是关于$\\mathbf{\\theta}$的假设函数，$\\mathbf{\\theta} = [\\theta_0;\\theta_1;…;\\theta_n]$，$\\mathbf{x} = [x_0, x_1, x_2, …, x_n]$并且$x_0=1$。 我们的目标当然是想要所有预测值都跟实际值相等，因此，目标函数或者称代价函数建模为： \\text{MSE}(\\mathbf{X},h_{\\mathbf{\\theta}}) = \\frac{1}{m}\\sum_{i=1}^{m}\\left(\\mathbf{\\theta}^T\\cdot\\mathbf{x}^{(i)}-y^{(i)}\\right)^2其中$\\mathbf{X}$为所有样本的集合，数量为$m$。 标准闭式解 基于均方误差最小化来进行模型求解的方法称为“最小二乘法”（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。 一般地，我们利用最小二乘法对$\\mathbf{\\theta}$进行估计。数据集$\\mathbf{X}$表示为： \\mathbf{X}=\\left( \\begin{array}{ccc} 1 & x_{11} & x_{12} & \\cdots\\ & x_{1d}\\\\ 1 & x_{21} & x_{22} & \\cdots\\ & x_{23} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{m1} & x_{m2} & \\cdots\\ & x_{md}\\\\ \\end{array} \\right) =\\left( \\begin{array}{ccc} 1 & \\mathbf{x}_1^T \\\\ 1 & \\mathbf{x}_2^T \\\\ \\vdots & \\vdots \\\\ 1 & \\mathbf{x}_m^T \\\\ \\end{array} \\right) 然后把实际值也写成向量模式$\\mathbf{y}=(y_1;y_2;…;y_m)$，因此优化问题为： \\mathbf{\\theta}^*=\\underset{\\mathbf{\\theta}}{\\text{arg}\\min}(\\mathbf{y}-\\mathbf{\\theta}^T\\mathbf{X})^T(\\mathbf{y}-\\mathbf{\\theta}^T\\mathbf{X}) 令$E_{\\mathbf{\\theta}}=(\\mathbf{y}-\\mathbf{\\theta}^T\\mathbf{X})^T(\\mathbf{y}-\\mathbf{\\theta}^T\\mathbf{X})$，对$\\mathbf{\\theta}$求导可得： \\frac{\\partial E_{\\mathbf{\\theta}}}{\\partial \\mathbf{\\theta}}=2\\mathbf{X}^T(\\mathbf{\\theta}^T\\mathbf{X}-\\mathbf{y}) 令上式等于0可解得 \\mathbf{\\theta}^*=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} 如果$\\mathbf{X}^T\\mathbf{X}$不可逆怎么办，这可能是由于矩阵中存在冗余特征，在线性代数里说明矩阵并非线性不相关，因此可以删除多余特征；也有可能是由于特征数大于等于样本数，也就是$m \\le n$，可以通过删除一些特征或者使用正则化（regularization，后续介绍）。 给出python一个实现例子如下： 12345678910111213141516171819import numpy as np X = 2 * np.random.rand(100, 1)y = 2 + 6 * X + np.random.randn(100, 1)X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instancetheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)print(theta_best)# [[1.74568681] [6.27246194]]X_new = np.array([[0], [2]])X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instancey_predict = X_new_b.dot(theta_best)print(y_predict) #[[ 1.74568681] [14.29061069]]plt.plot(X_new, y_predict, &quot;r-&quot;)plt.plot(X, y, &quot;b.&quot;)plt.axis([0, 2, 0, 15])plt.show() 如果用sklearn实现的话，代码如下：12345678&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; lin_reg = LinearRegression()&gt;&gt;&gt; lin_reg.fit(X, y)&gt;&gt;&gt; lin_reg.intercept_, lin_reg.coef_(array([ 4.21509616]), array([[ 2.77011339]]))&gt;&gt;&gt; lin_reg.predict(X_new)array([[ 4.21509616],[ 9.75532293]]) 这种方式实现的计算复杂度只要在矩阵求逆上，对于一个$n \\times n$的矩阵，求逆复杂度大约是$O(n^{2.4})$到$O(n^{3})$，当特征数量很大时（比如100,000以上时），标准闭式解会变得很慢；另一方面，对于样本数量其复杂度是$O(m)$，也就是线性的；此外，模型训练完成后，预测的复杂度对于样本数量和特征数量复杂度都很快。 梯度下降法 梯度下降法是十分常用的优化算法，目的是通过迭代过程不断更新参数进而最小化代价函数，每次优化的方向都是斜率绝对值最大的方向。Note： 需要谨慎选择步长（step size）或者叫做学习率（learning rate），太小的话，需要许多次迭代才能走到最优点，而步子太大，则有可能跨过最优点，然后陷入震荡。 凸优化问题找到最优点比较容易，而非凸问题则容易陷入局部最优点或者陷入平台（plateau)。线性回归问题是个凸优化问题。 最好在使用梯度下降法前对数据进行标准化处理，不然“碗”太长需要更多的迭代次数。 问题参数越多，维度越大，参数空间越大，搜索最优解也就越难。 Batch Gradient Descent Batch是批的意思，批梯度下降就是在每次迭代过程中把所有数据都来计算代价函数的偏导，也就是梯度。代价函数对于参数$\\theta_j$的偏导为： \\frac{\\partial \\text{MSE}(\\theta)}{\\partial{\\theta}_j}=\\frac{2}{m}\\sum_{i=1}^{m}\\left(\\theta^T \\cdot \\mathbf{x}^{(i)}-y^{(i)}\\right)x_j^{(i)} 用向量来表示梯度下降为： \\nabla_{\\theta}\\text{MSE}(\\theta)=\\left( \\begin{array}{ccc} \\frac{\\partial \\text{MSE}(\\theta)}{\\partial \\theta_0}\\\\ \\frac{\\partial \\text{MSE}(\\theta)}{\\partial \\theta_1}\\\\ \\vdots\\\\ \\frac{\\partial \\text{MSE}(\\theta)}{\\partial \\theta_0} \\end{array} \\right) =\\frac{2}{m}\\mathbf{X}^T\\cdot(\\mathbf{X}\\cdot \\theta-\\mathbf{y}) 由此，我们可以得到梯度下降的步骤： \\theta^{(n+1)} = \\theta^{(n)}-\\eta \\nabla_{\\theta}\\text{MSE}(\\theta) 推导出这个公式之后就可以实现了：1234567eta = 0.1 # learning raten_iterations = 1000m = 100theta = np.random.randn(2,1) # random initializationfor iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients Stochastic Gradient Descent 在前面讲过，BGD在每次迭代中是把所有数据都用于计算的，因此当数据量很大时，算法会变得难以忍受的慢，因此需要使用其他梯度下降算法。随机梯度下降法跟批梯度下降是两个极端，SGD在每次迭代中会随机从训练集中选择一个样本，然后只用这个样本计算并更新梯度。因此，这个算法计算比BGD快很多很多；但也由于其随机特性，优化曲线不会“直接”向着最优点前进，而是来回波动，但最终会抵达最优值附近。其算法如下图所示： Python实现如下：1234567891011121314n_epochs = 50t0, t1 = 5, 50 # learning schedule hyperparametersdef learning_schedule(t):return t0 / (t + t1)theta = np.random.randn(2,1) # random initializationfor epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch * m + i) theta = theta - eta * gradientsprint(theta) 如果用sklearn来实现的话，代码就简单多了：1234from sklearn.linear_model import SGDRegressorsgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)sgd_reg.fit(X, y.ravel())print(sgd_reg.intercept_, sgd_reg.coef_) NOTE: epoch是指把所有数据样本都遍历一遍，iteration表示更新一次参数，batch则是指数据样本的大小，在SGD中，iteration=batch=1，epoch=num_X/batch Mini-batch Gradient Descent Mini-batch GD介于SGD和BGD之间，也就是每次用来更新参数的样本不是一也不是所有（吴恩达说在2-400之间，他倾向于用10），它吸收了BGD和SGD的优缺点的折中，收敛更快，随机性较小，但也难抵达最优值，其算法流程如下图所示： 这里介绍的标准闭式解只适用于线性回归问题，但梯度下降法可以用来训练许多模型，在深度学习中也有广泛应用，将他们比较如下： Algorithm Large m Out-of-core Support Large n Hyperparameters Scaling Required Sklearn Normal Equation Fast No Slow 0 No LinearRegression Batch GD Slow No Fast 2 Yes n/a Stochastic GD Fast Yes Fast $\\ge$2 Yes SGDRegressor Mini-batch GD Fast Yes Fast $\\ge $2 Yes n/a Polynomial Regression 如果数据并非一条直线呢，那么还可以用线性模型吗？实际上是可以的，主要方法就是把一个特征拓展成多维的，比如$x^2, x^3,…, x^k$，然后将其作为另一个维度的特征，比如$x_1 = x, x_2 = x^2$，那么就可以用线性模型按照上面的方法进行训练了，用sklearn实现如下： 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.linear_model import LinearRegressionm = 100X = 6 * np.random.rand(m, 1) - 3y = 1 * X**2 + X + 2 + np.random.randn(m, 1)plt.scatter(X, y)poly_features = PolynomialFeatures(degree=2, include_bias=False)X_poly = poly_features.fit_transform(X)lin_reg = LinearRegression()lin_reg.fit(X_poly, y)print(lin_reg.intercept_, lin_reg.coef_)xx = np.array((range(-3, 4)))print(xx)yy = xx*0.98134428+xx*xx*0.94365742+1.94555083plt.plot(xx, yy, c=&apos;r&apos;)plt.show() 此外，需要注意，PolynomialFeatures(degree=d)会把一个包含n个特征的矩阵转化为包含$\\frac{(n+d)!}{d!n!}$个特征的矩阵，即如果对于两个特征a和b，令degree=3，那么特征不仅有$a^2$，$a^3$，$b^2$，$b^3$，还会有$ab$，$a^2b$和$ab^2$.","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dlc1994.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dlc1994.github.io/tags/Python/"},{"name":"Linear Regression","slug":"Linear-Regression","permalink":"https://dlc1994.github.io/tags/Linear-Regression/"},{"name":"Polynomial Regression","slug":"Polynomial-Regression","permalink":"https://dlc1994.github.io/tags/Polynomial-Regression/"},{"name":"Gradient Decent Methods","slug":"Gradient-Decent-Methods","permalink":"https://dlc1994.github.io/tags/Gradient-Decent-Methods/"}]},{"title":"Introduction to Machine Learning","slug":"Introduction","date":"2018-11-28T12:13:53.000Z","updated":"2019-03-17T10:09:57.923Z","comments":true,"path":"Introduction.html","link":"","permalink":"https://dlc1994.github.io/Introduction.html","excerpt":"做了与机器学习相关的项目好久了，但对机器学习一直没有一个系统的认识，导致在切入一些新的领域时力有不逮，总感觉理解有偏差或者理解困难，因此想系统地、详细地学习机器学习，就从周志华的西瓜书和Andrew Ng的机器学习视频开始吧。","text":"做了与机器学习相关的项目好久了，但对机器学习一直没有一个系统的认识，导致在切入一些新的领域时力有不逮，总感觉理解有偏差或者理解困难，因此想系统地、详细地学习机器学习，就从周志华的西瓜书和Andrew Ng的机器学习视频开始吧。 众所周知， 机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新是研究计算机怎样模拟或实现人类的学习行为。 来自卡内基梅隆大学的Tom Mitchell提出的关于机器学习的定义较为人所接受，他定义的机器学习是，一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。 一般而言，我把机器学习分为监督学习、无监督学习和增强学习，而深度学习和大数据是拓展也是紧密的关联，如下图所示： 监督学习，顾名思义，就是有人指导你，告诉你这个学习正确与否。专业点来讲，就是数据集中每个样本都会带有一个正确答案，比如在西瓜分类里，“红瓤，有蒂”带有标签“甜”，“黄瓤，无蒂”带有标签“不甜”；在回归里，房价预测里每个样本都带有房价。分类和回归的区别在于预测的是一组离散的结果，而回归可以预测连续的输出。 无监督学习，从上图可以看出，我们事先不知道样本是哪一类或者是哪一个值，也就是没有任何的标签或“答案”。一个比较著名的例子就是鸡尾酒宴问题：许多人坐在一起参加鸡尾酒会，大家都在同一时间说话，声音此起彼伏，重重叠叠，要想分离出不同人说话的声音，就是一个聚类问题，也是无监督学习问题。这个例子可能比较难懂，再说一个就是在无线通信中，有时会根据用户的行为特征对用户进行分类，便于进行一些资源的调度和统一管理，而如何判断一堆用户里哪些用户是类似的，比如上班族、学生党、旅客等，这就是一个典型的聚类问题。 增强学习则是要解决这样的问题，一个能感知环境的自治agent，怎样通过学习选择能达到其目标的最优动作。当agent（机器人，下棋，在无线领域则可以是用户、基站和operator）在环境中作出某个动作时，会产生不同的奖励值或者惩罚值，agent的任务就是从这个非直接的，有延迟的回报中学习，以便后续的动作产生最大的累积效应。 在后续的文章中，我们会对各个算法有比较详细的学习过程，敬请期待啦。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://dlc1994.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dlc1994.github.io/tags/Python/"}]},{"title":"LeetCode Solutions","slug":"LeetCode Solutions","date":"2018-07-30T04:50:07.000Z","updated":"2019-03-17T10:09:34.579Z","comments":true,"path":"LeetCode Solutions.html","link":"","permalink":"https://dlc1994.github.io/LeetCode Solutions.html","excerpt":"Here are my solutions to the LeetCode problems, linking to my Github repository:","text":"Here are my solutions to the LeetCode problems, linking to my Github repository: EASY Jewels and Stones Unique Morse Code Words Circle Flipping an Image Hamming Distance To Lower Case Two Sum MEDIUMHARD","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://dlc1994.github.io/categories/LeetCode/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dlc1994.github.io/tags/Python/"},{"name":"Algorithms","slug":"Algorithms","permalink":"https://dlc1994.github.io/tags/Algorithms/"}]},{"title":"Water Filling Algorithm and Matlab Simulation","slug":"Water-Filling-Algorithm-and-Matlab-simulation","date":"2018-07-18T15:33:07.000Z","updated":"2018-07-21T08:13:06.088Z","comments":true,"path":"Water-Filling-Algorithm-and-Matlab-simulation.html","link":"","permalink":"https://dlc1994.github.io/Water-Filling-Algorithm-and-Matlab-simulation.html","excerpt":"","text":"注水算法是根据某种准则，并根据信道状况对发送功率进行自适应分配，通常是信道状况好的时刻，多分配功率，信道差的时候，少分配功率，从而最大化传输速率。 当接收端完全已知CSI（信道状态信息）而发送端未知CSI时，发送天线阵列中的功率平均分配是合理的。 实现功率的“注水”分配，发送端必须知道CSI。 直观而言，就如下图所示： 图1 注水原理示意图（白色平台越高代表信道条件越差，注入的水就越少） 注水原理可以建模为下述优化问题： \\begin{align} & \\underset{P_1,P_2,...,P_N}{\\mathop{\\max }}\\,\\text{ }{C_{sum}}=\\sum_{n=1}^{N}{\\log \\left( 1+\\frac{P_n{\\left| {h_n} \\right|}^2}{N_0} \\right)} \\\\ & \\text{subject to }\\sum_{n=1}^{N}{P_n}={P}_{sum},n=1,2,...,N \\ \\end{align}其中${C}_{sum}$表示系统总信道容量，$N$为信道数，${P}_{n}$为第$n$个信道的功率，${h}_{n}$为第$n$个信道的信道增益，${N}_{0}$为噪声功率谱密度，${P}_{sum}$为传输总功率，也就是总水量。 该优化问题为凸优化问题，可以用拉格朗日乘数法求得全局最优解。 \\mathcal{L}(\\lambda ,{P_1},{P_2},...,{P_N})=\\sum_{n=1}^{N}{\\log \\left( 1+\\frac{P_n{\\left| {h_n} \\right|}^2}{N_0} \\right)}+\\lambda (\\sum_{n=1}^{N}{P_n}-P_{sum})令$\\frac{\\partial \\mathcal{L} }{\\partial P_n}=\\frac{\\partial \\mathcal{L} }{\\partial \\lambda }=0$，解得最优功率分配方案为： P_{n}^{*}={\\left( \\frac{1}{\\lambda }-\\frac{N_0}{\\left| {h}_{n} \\right|}^{2} \\right)}^{+}其中${(\\centerdot )}^{+}$表示取值非负。 MATLAB实现 1234567891011121314151617181920212223242526272829303132333435363738394041clear all;channel_n=10; %channel numberM=[5,10,20,50]; %transmitted powerN0=0.5; h_1= random(&apos;rayleigh&apos;,1,1,channel_n); %Rayleigh fadingh_2=h_1.^2; %|h|.^2h_2_sorted=sort(h_2); %sort the channel gainh=h_2_sorted/N0; syms lamdafor m=1:length(M) fprintf(&apos;transmitted power is %d watt&apos;,M(m)); for k=1:channel_n p=zeros(1,channel_n); sum=0; for i=k:channel_n sum=sum+(1/lamda-1/h(i)); end f=sum-M(m); x=solve(f,lamda); %find lamda if k==1 if vpa(x)&gt;0 &amp;&amp; vpa(x)&lt;h(k) for i=k:channel_n p(i)=1/x-1/h(i); %allocate power capacity=capacity+log2(1+p(i)*h(i));%compute the capacity end p(i)=vpa(p(i),3) capacity=vpa(capacity,3) end else if vpa(x)&gt;h(k-1) &amp;&amp; vpa(x)&lt;h(k) capacity=0; for i=k:channel_n p(i)=1/x-1/h(i); %allocate power capacity=capacity+log2(1+p(i)*h(i)); %compute the capacity end p(i)=vpa(p(i),3) capacity=vpa(capacity,3) end end endend","categories":[{"name":"Wireless Communications","slug":"Wireless-Communications","permalink":"https://dlc1994.github.io/categories/Wireless-Communications/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"https://dlc1994.github.io/tags/Algorithms/"},{"name":"Matlab","slug":"Matlab","permalink":"https://dlc1994.github.io/tags/Matlab/"}]},{"title":"How to Make a Visual Mark Up and Revision of Significant Differences between Two Latex Files by Latexdiff","slug":"Latexdiff","date":"2018-06-19T15:34:10.000Z","updated":"2018-07-21T08:12:50.459Z","comments":true,"path":"Latexdiff.html","link":"","permalink":"https://dlc1994.github.io/Latexdiff.html","excerpt":"","text":"Recently my survey was rejected and required minor revision, while I have to submit a revised article with the “editing mode” feature turned on. Because I use Miktex+Texstudio to edit my paper in windows 10, Latexdiff is considered of course. Latexdiff is a Perl script for visual mark up and revision of significant differences between two LATEX files. Various options are available for visual mark up using standard LATEX packages such as color. Changes not di­rectly af­fect­ing vis­i­ble text, for ex­am­ple in for­mat­ting com­mands, are still marked in the LATEX source. A rudi­men­tary re­vi­sion fa­cilil­ity is pro­vided by an­other Perl script, la­texre­vise, which ac­cepts or re­jects all changes. Man­ual edit­ing of the dif­fer­ence file can be used to over­ride this de­fault be­haviour and ac­cept or re­ject se­lected changes only. Step 1:For a pure rookie, first of all, you need to install CTEX (for Chinese users) or Miktex. Step 2:Then, win+R and open cmd, input command latexdiff, if returns 12 and only 2 non-option arguments required. Write latexdiff -h to get help Congrats! You have successfully installed latexdiff and you can turn to Step 3.3. Step 3:Otherwise, don’t worry, I encountered the same situation as you. Do as follows. 3.1 Install Latexdiff PackageAfter you install a Tex, open Start Menu-&gt;Ctex or Miktex-&gt;Package Manager (Admin). Search Latexdiff in column Name: as follows Click the + button and install it. 3.2 Install PerlLatexdiff is a Perl script, so you can go to Perl download corresponding version in your computer. Technically, ActivePerl and StrawberryPerl is both ok. Same steps in Step 2, if you have problem as 12latexdiff: The Perl script could not be found. latexdiff: Data: scripts/latexdiff/perl/latexdiff.pl It’s a common problem and you can find the answer in 这里 or Here. It is worth mentioning that if you have the following problem which confused me for a long time 12latexmk: the script engine could not be found latexdiff: data: scriptengine=&quot;perl.exe&quot; Actually I have no idea how I successfully solve it. You may do the trials as what I have done. Check the System Path of Perl and Tex, especially according to x86 or x64 Reinstall Perl, if not work, change to ActivePerl or StrawberryPerl Restart your windows after you make some changes 3.3 LatexdiffPut your origin tex version (e.g., old.tex) and your modified tex version (e.g., new.tex) in a same file. cmd to this path, and input 1latexdiff old.tex new.tex &gt;diff.tex where diff.tex is the editted version, add the required files to this path and compile it. Then you can have the results If you encounter some errors in compiling, just google it because they are just some common LATEX errors. Enjoy. !!!Tips: if you suffer any problem, first go to google or baidu it. Please discover the truth by yourself.","categories":[{"name":"Paper Work","slug":"Paper-Work","permalink":"https://dlc1994.github.io/categories/Paper-Work/"}],"tags":[{"name":"Latex","slug":"Latex","permalink":"https://dlc1994.github.io/tags/Latex/"}]},{"title":"A Stupid Mistake I Made about Sorting in Python DataFrame","slug":"A-Stupid-Mistake-I-Made-about-Sorting-in-Python-DataFrame","date":"2018-06-12T02:47:31.000Z","updated":"2018-07-21T08:12:27.271Z","comments":true,"path":"A-Stupid-Mistake-I-Made-about-Sorting-in-Python-DataFrame.html","link":"","permalink":"https://dlc1994.github.io/A-Stupid-Mistake-I-Made-about-Sorting-in-Python-DataFrame.html","excerpt":"","text":"One day when I try to sort a DataFrame by a column, an amazing mistake happens! I will reproduce this stupid thing here. Firstly, make a dataframe example: 12345 a b c0 9 4 61 2 7 52 5 -3 83 1 2 3 123frame = pd.DataFrame(&#123;&quot;a&quot;:[9,2,5,1],&quot;b&quot;:[4,7,-3,2],&quot;c&quot;:[6,5,8,3]&#125;)frame.sort_values(&apos;a&apos;,inplace=True)print(frame) What do you think the result will be? What I expect it will get is like this: 12345 a b c3 1 2 31 2 7 52 5 -3 80 9 4 6 However, what I actually get is 12345 a b c0 9 4 61 2 7 52 5 -3 83 1 2 3 I get really confused, so I try all the arg in function DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’) I find out that only if the inplace= is set to True, the result is as expected. But the usage of this function I searched in google, did not meantion this parameter. Therefore, I look for an instruction of inplace， and I find that the inplace parameter is a generic term w.r.t pandas and not specific to sort_values alone. You can see it in several functions like pd.fillna, pd.replace etc. Whenever the inplace is set to True, it modifies the existing data frame and you need not assign it to a new data frame. Ohhhh… Then I find out where the mistake really lies in. In my previous code, the DataFrame frame I sorted has not been modified only if the parameter inplace is set to True, so I modify the code as follow: 123frame = pd.DataFrame(&#123;&quot;a&quot;:[9,2,5,1],&quot;b&quot;:[4,7,-3,2],&quot;c&quot;:[6,5,8,3]&#125;)df = frame.sort_values(&apos;a&apos;,inplace=True)print(df) The problem is solved! How stupid I was!","categories":[{"name":"Data Processing","slug":"Data-Processing","permalink":"https://dlc1994.github.io/categories/Data-Processing/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://dlc1994.github.io/tags/Python/"},{"name":"DataFrame","slug":"DataFrame","permalink":"https://dlc1994.github.io/tags/DataFrame/"},{"name":"Sorting","slug":"Sorting","permalink":"https://dlc1994.github.io/tags/Sorting/"}]},{"title":"Hexo: setup a static blog","slug":"Hexo-setup-a-static-blog","date":"2018-06-12T01:46:53.000Z","updated":"2018-07-21T08:12:39.650Z","comments":true,"path":"Hexo-setup-a-static-blog.html","link":"","permalink":"https://dlc1994.github.io/Hexo-setup-a-static-blog.html","excerpt":"","text":"​ Before we start, you have to know what is the difference between static site and dynamic site. Let’s take a look at the definition in Wiki: A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored, in contrast to dynamic web pages which are generated by a web application. Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a web server to negotiate content-type or language of the document where such versions are available and the server is configured to do so. It’s pros and cons are listed as follows: Advantages of a static website Provide improved security over dynamic websites Improved performance for end users compared to dynamic websites Fewer or no dependencies on systems such as databases or other application servers Disadvantages of a static website Dynamic functionality has to be added separately ​ Dynamic site is not our topic here. I wil talk about how to deploy a hexo blog in Github repository under the environment of Window 10. 1. Github initialization​ From the very beginning, you have to create a GitHub account and new a repository, note that the repository name need to be the same with your owner name (the alarm is appeared cause I have already newed a same one). REMEMBER the repository address (two kinds of address—- SSH: git@github.com:dlc1994/dlc1994.github.io.git and HTTPS: https://github.com/dlc1994/dlc1994.github.io.git). The configuration of SSH can be seen in here, and HTTPS has no need of extra operation. Install Git tool and the rest of operations is done through this tool. 2. Install the necessary componentsAfter that, install Node.js, in case of the low speed, you can also download it in here. Now it’s time to install Hexo in your machine, click right-hand button anywhere and open Git Bash, then input 12npm install hexo-cli -gnpm install hexo-deployer-git --save Wait a minute you can check if the component is installed successfully. Input directly in Git Bash, or Win+R and input cmd, then input codes as follow: 123git --versionnode -vnpm -v If return the version number then you can go on, otherwise just google your mistake during your installation. After Hexo is installed, go to the place where you want to save your blog and open Git Bash. Input hexo init, you can find that a lot of files are created. 3. Create your first blogUse file editor like Notepad++, open file _config.yml, find the codes and edit it as you want 123456789101112131415# Sitetitle: DLC&apos;s Blog #your blog namesubtitle: Find myself in the darkest place #your blog subtitledescription: Personal Blog #description of your blogkeywords:author: language: timezone: Asia/Shanghaitheme: landscape #your blog theme, I will talk about how to change it to `Next`deploy: type: repository: git@github.com:dlc1994/dlc1994.github.io.git #your repository address branch: master In Git Bash, input hexo g and hexo s, it will return (perfectly) Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. Open your browser and input localhost:4000 in address bar, you can see that (Ok it is really ugly) Lastly, you have to deploy it to your github repository, continue to input hexo d. Luckily, there is no error and you can see it in your repository address like git@github.com:dlc1994/dlc1994.github.io.git. If not, go to google for help. Create a new blog use hexo new &quot;postname&quot; or in your blog path blogpath\\source\\_posts new a .md file and edit it use Markdown (Typora is recommended here). More commands can be found here. I will update the process of blog beautification in my next blogs. !!!Tips: if you suffer any problem, first go to google or baidu it. Please discover the truth by yourself.","categories":[{"name":"HEXO","slug":"HEXO","permalink":"https://dlc1994.github.io/categories/HEXO/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://dlc1994.github.io/tags/Hexo/"},{"name":"Markdown","slug":"Markdown","permalink":"https://dlc1994.github.io/tags/Markdown/"}]}]}